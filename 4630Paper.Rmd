---
title: "Predicting Breast Cancer Survival Using Clinical and Genomic Features: A Machine-Learning Analysis of the METABRIC Cohort"
author: "Your Names"
date: "December 9, 2025"

output:
  pdf_document:
    latex_engine: xelatex
    number_sections: true
    toc: false
    includes:
      in_header: preamble.tex

geometry: margin=1in
fontsize: 11pt
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo   = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.width  = 5.5,
  fig.height = 3.6,
  fig.align  = "center"
)

```

Abstract

Breast cancer survival illustrates the influence of clinical severity, demographic context, and underlying genomic structure. Predictive modeling techniques in this domain require handling heterogeneous variable types, extensive missingness, strong correlations among tumor severity markers, and a genomic block composed of hundreds of extremely sparse mutation indicators. The METABRIC cohort contains more than 1900 patients and nearly 700 baseline variables, providing a setting in which these challenges appear together. Exploratory analysis revealed a moderate imbalance in the survival outcome, substantial missingness in tumor stage, meaningful missingness in histologic grade and tumor size, and widespread sparsity among gene mutation indicators. Strong correlation among tumor size, lymph node involvement, the Nottingham Prognostic Index, and histologic grade demonstrated that these features represent overlapping manifestations of disease burden.
These data characteristics motivated a preprocessing pipeline that emphasized extensive data cleaning, explicit handling of missing values, removal of extreme outliers in clinical predictors, merging of rare categories in categorical variables, median imputation for numeric variables, filtering of predictors with very low variation, dimensionality reduction of sparse mutation indicators through principal component analysis, and a focused set of clinically informed interaction terms. Earlier stages of feature construction produced a larger set of engineered predictors, yet the final design matrix contained about 28 predictors that combined cleaned clinical variables, a small set of principal components derived from mutation indicators, and only the most meaningful clinically grounded interactions. The purpose of this process was not simply to reduce dimensionality but to develop a feature space that allowed statistical models to reflect the biological structure of breast cancer rather than the irregularities and noise present in raw measurements.
Three machine learning models were trained, specifically a linear support vector machine, a Random Forest model, and XGBoost. Training used a stratified split that reserved 30 percent of the data for testing, along with repeated 5-fold cross-validation that optimized the area under the receiver operating characteristic curve. Model performance was evaluated using accuracy, the area under the curve, macro F1, predicted probability distributions, variable importance figures, individual and partial dependence plots, and calibration assessments. XGBoost yielded the strongest ranking ability, Random Forest achieved the most balanced performance across outcome classes, and the linear support vector machine produced the most stable and conservative calibration. Across models, age at diagnosis consistently emerged as the dominant predictor, while principal components derived from mutation indicators captured genomic variation not reflected in clinical severity markers. Overall, the differences among the models show the importance of preprocessing decisions, dimensionality reduction, and clinically informed feature engineering for the modeling of breast cancer survival in the METABRIC cohort.


Introduction

Breast cancer survival emerges from biological processes operating across clinical, demographic, and genomic levels. Tumor burden, lymphatic involvement, treatment history, and genomic instability all influence survival probability. These processes do not act independently because age modifies tumor biology, tumor biology modifies treatment response, and genomic alterations shape whether tumors follow more aggressive or less aggressive growth patterns. This interdependence explains why methods that model only isolated linear effects often fail to represent the full biological landscape. Machine learning methods are increasingly used to model these interactions because they can integrate large numbers of clinical features with high-dimensional genomic measurements and can capture nonlinear, conditional, and threshold-driven effects that more restrictive models cannot represent. The METABRIC cohort contains extensive clinical information together with several hundred binary mutation indicators and therefore provides a rich environment for understanding how models interpret joint clinical and genomic structures. The dataset captures many elements of contemporary oncology practice, including tumor morphology, detailed staging components, hormone receptor information, lymphatic involvement, and patient demographics. The genomic block adds another layer by indicating the presence or absence of somatic mutations that may influence tumor behavior through pathway dysregulation, replication stress, or deficiencies in DNA repair. These mutation indicators are extremely sparse because many mutations occur only in small subsets of tumors, reflecting the biological reality that breast cancer involves a small set of common pathways along with a much larger set of rare or private alterations.
The subset used in this study included 1904 patients with complete overall survival information and follow-up time. Survival status was treated as a binary endpoint representing whether the patient was alive at the final follow-up. The initial set of nearly 700 predictors included clinical, demographic, histologic, and surgical variables, together with several hundred mutation indicators derived from sequencing. Clinical variables such as tumor size, lymph nodes examined positively, the Nottingham Prognostic Index, histologic grade, age at diagnosis, and tumor stage capture different dimensions of disease burden and aggressiveness. However, these variables overlap substantially because they reflect downstream manifestations of tumor progression. The mutation indicators provide a high-dimensional yet sparse representation of genomic alterations that signal instability, pathway disruption, or distinct tumor phenotypes. Their relationship with survival is subtle and frequently interacts with clinical severity rather than functioning as isolated predictors. Exploratory analysis revealed structural challenges that shaped the modeling strategy. Tumor stage had the highest missingness, with more than 25 percent of patients lacking stage information. Histologic grade and tumor size showed moderate missingness. The mutation block exhibited extreme sparsity, with most genes appearing in only small percentages of tumors. Correlation heatmaps confirmed substantial overlap among traditional severity variables, including tumor size, lymph node involvement, the Nottingham Prognostic Index, and histologic grade. These correlations reflect shared biological processes. In contrast, genomic indicators showed little correlation with clinical variables, suggesting that they represent a largely independent dimension of risk. This motivated a preprocessing pipeline that handled clinical and genomic domains differently, rather than forcing them into a common structure. Without careful preprocessing, missingness could bias model estimates, sparsity could distort model structure, and correlated predictors could destabilize decision boundaries. The aim of the study was therefore to build a predictive pipeline anchored in exploratory findings, designed to reduce noise and redundancy, and capable of integrating both clinical severity and genomic structure. The analysis sought not only to compare predictive performance but to explain how different modeling frameworks interpret breast cancer risk patterns in METABRIC.

Methodology

All analyses were performed in R using packages designed for data cleaning, modeling, and validation. Data cleaning was conducted with dplyr and tidyr, and exploratory visualization was carried out with ggplot2 and corrplot. Predictive modeling used ranger for Random Forests, xgboost for gradient boosting, and a linear support vector machine trained through caret with sparse matrices produced by the Matrix package. Predictors that directly encoded survival information, including follow-up time and death-related variables, were removed to prevent leakage of outcome information. Categorical variables were cleaned, recoded, and given explicit missing levels. Rare categories were merged to avoid fragmentation and instability. Numeric clinical variables were inspected for skewness, irregular distributions, and extreme values. Outliers in tumor size, lymph nodes examined positively, and other continuous predictors were removed because exploratory analysis showed that these values distorted decision surfaces and inflated variance. These extreme observations were rare and biologically implausible, suggesting transcription errors instead of meaningful clinical patterns.
Missing numeric values were imputed with the median within the training set in order to avoid leakage. Predictors with extremely limited variation were removed, especially within the mutation block, where many genes showed little activity. Several clinically relevant predictors, including tumor stage, the Nottingham Prognostic Index, and histologic grade, were excluded despite their biological significance. Tumor stage had substantial missingness that made imputation unreliable. Histologic grade and the Nottingham Index were nearly perfectly correlated with tumor size and lymph node burden, creating redundancy and destabilizing model behavior. Diagnostic assessments confirmed that retaining these variables offered no measurable benefit once outlier removal and filtering created a more compact representation of severity. To incorporate genomic structure without replicating the sparsity of the mutation matrix, principal component analysis was applied to the filtered mutation indicators. Only components explaining meaningful variation were retained. A limited group of clinically motivated interactions was also included, reflecting patterns observed during exploratory analysis. After integrating these elements, the final predictor matrix contained about 28 variables.

Evaluation

Evaluation of the three models revealed not only differences in predictive performance but also important distinctions in how each method interacted with the clinical and genomic structure of the dataset. The support vector machine, the Random Forest model, and XGBoost achieved broadly similar numerical metrics, but the reasons for these similarities and differences were shaped by their distinct modeling assumptions. The linear support vector machine reached an accuracy of 0.6976, an area under the curve of 0.7554, and a macro F1 of 0.68. Random Forest reached an accuracy of 0.7086, an area under the curve of 0.7669, and a macro F1 of 0.695. XGBoost showed accuracy near 0.7031 and an area under the curve near 0.7653, with a macro F1 slightly lower than Random Forest. Although these differences appear modest, they become meaningful when interpreted in the context of breast cancer.
Breast cancer severity does not change smoothly across predictors, even for traditional features such as tumor size or lymph nodes examined positively. These clinical measures reflect biological processes that often produce abrupt changes in risk, for example, when additional lymphatic involvement signals a transition to more advanced disease. A linear support vector machine can approximate these patterns only by stretching a single separating plane. Even with clinically informed interactions, it cannot fully represent situations where genomic structure modifies the effect of tumor burden. This explains the compressed predicted probability distributions and conservative calibration displayed by the model. It captures broad clinical gradients but smooths away the nonlinear structure present in the dataset.
Random Forest differs because it splits data into smaller groups that can align more closely with biological thresholds. Its slightly higher accuracy and macro F1 reflect its ability to capture changes in survival risk that occur once severity crosses certain clinical boundaries, such as higher lymphatic involvement. The ensemble structure allows it to approximate many clinical pathways at once, which is useful in a cohort where tumor progression, hormone receptor profiles, and age-related risk interact. The differing learning mechanics of the models produced distinct patterns of variable importance that shaped their predicted probabilities. The linear support vector machine placed most weight on broad clinical predictors because its single separating plane favors variables that define a nearly linear severity axis. The tree-based models, in contrast, emphasized the principal components representing mutation-derived variation because these components capture nonlinear interactions and sparse genomic signals that a linear method cannot represent. Random Forest aggregated many such patterns, while XGBoost amplified them by repeatedly focusing on patients whose outcomes deviated from the standard relationship between clinical severity and survival. These deviations often reflect biologically distinctive tumor subtypes, which allowed XGBoost to highlight subtle genomic influences. As a result, the tree-based models captured fine-grained heterogeneity that the support vector machine smoothed over, producing sharper contrasts in probability assignments where genomic structure was influential. Both the similarities and differences across models reflect their underlying assumptions. The support vector machine emphasized global severity trends, Random Forest emphasized threshold-based structure, and XGBoost emphasized localized genomic and clinical deviations.

Discussion

The comparative behavior of the models shows how statistical assumptions interact with the biological and clinical structure of breast cancer. Although age at diagnosis, tumor size, and lymph node involvement remained dominant predictors across all models, each method integrated these variables in different ways, illustrating how modeling frameworks shape biological interpretation. These predictors dominate because they capture broad physiological processes that govern tumor growth, metastatic potential, immune system decline, and constraints on treatment. Their stability and completeness in METABRIC further reinforced their influence across learning methods.
The linear support vector machine demonstrates how a strong preference for global linear structure shapes the representation of clinical severity. The model treats increases in tumor burden or age as producing uniform changes in survival probability across all patients. Clinically, however, risk escalation depends on tumor biology, patient age, proliferation patterns, and genomic instability. When the effect of tumor size depends on genomic context, a linear model approximates this interaction in an averaged way. This produces predictions that are biologically conservative and stable yet unable to represent distinctive tumor subtypes or nuanced genomic influences. Random Forest approximates clinical decision-making by relying on threshold-based rules. In oncology, a small change in lymph node burden can move a patient into a different prognostic group. A slight increase in tumor size may matter little unless combined with specific receptor characteristics or age groups. The structure of decision trees captures these biological thresholds and therefore identifies clinically meaningful subgroups. This allows Random Forest to represent heterogeneity in intermediate risk regions more clearly than a linear method. Yet the predicted probabilities remain coarse because they represent averages within discrete leaves rather than smooth biological processes. XGBoost amplifies patterns that deviate from classical severity markers. Its iterative focus on misclassified cases directs learning to combinations of genomic and clinical factors that yield survival outcomes unexpected from standard predictors. Many tumors in METABRIC contain genomic patterns that modify prognosis independently of tumor size or lymphatic involvement. XGBoost detects these subtypes by emphasizing residual errors and creates fine distinctions among patients who appear similar clinically but differ genetically. This makes the model more sensitive to biologically meaningful heterogeneity but also increases the sharpness of its probability surfaces.
Overall, the results show that survival in METABRIC is shaped by both broad severity gradients and localized genomic variation. The support vector machine captures the broad clinical trends, Random Forest captures threshold-driven structure that often reflects clinical staging and biological cut points, and XGBoost captures localized genomic deviations that influence prognosis within clinically similar groups. Additional gains may also come from deeper modeling of nonlinear clinical interactions, especially those involving tumor size, lymphatic involvement, receptor status, and age at diagnosis. More refined methods for summarizing genomic structure, such as pathway-level aggregation or regularized dimension reduction techniques, could provide a richer representation of mutation patterns without reintroducing sparsity. Expanding the modeling framework to incorporate time-to-event outcomes rather than binary survival status may also improve clinical interpretability. Taken together, these directions suggest that increasingly integrated and biologically informed modeling approaches are likely to improve prediction while offering clearer insights into the mechanisms that shape breast cancer survival in METABRIC.
